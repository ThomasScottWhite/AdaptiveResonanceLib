{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from artlib import TD_FALCON, FuzzyART, compliment_code\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "# only works with Fuzzy ART based FALCON\n",
    "def prune_clusters(cls):\n",
    "    # get existing state and action weights\n",
    "    state = np.array(cls.fusion_art.modules[0].W)\n",
    "    action = np.array(cls.fusion_art.modules[1].W)\n",
    "    # get predicted rewards for each cluster\n",
    "    reward = np.array(cls.fusion_art.get_channel_centers(2)).reshape((-1, 1))\n",
    "\n",
    "    # combine state and actions\n",
    "    combined = np.round(np.hstack((state, action)), decimals=3)\n",
    "    # identify unique combinations\n",
    "    unique_combined, indices = np.unique(combined, axis=0, return_inverse=True)\n",
    "    # get mean reward prediction for each unique state-action pair\n",
    "    unique_rewards = np.array([reward[indices == i, :].mean() for i in range(len(unique_combined))])\n",
    "\n",
    "    # split unique state-action pairs\n",
    "    unique_states = unique_combined[:, :state.shape[1]]\n",
    "    unique_actions = unique_combined[:, state.shape[1]:]\n",
    "\n",
    "    # update model to only have unique state-action pairs and their average rewards\n",
    "    cls.fusion_art.modules[0].W = [row for row in unique_states]\n",
    "    cls.fusion_art.modules[1].W = [row for row in unique_actions]\n",
    "    cls.fusion_art.modules[2].W = [row for row in compliment_code(unique_rewards.reshape((-1, 1)))]\n",
    "\n",
    "    return cls\n",
    "\n",
    "def update_FALCON(records, cls, shrink_ratio):\n",
    "    # convert records into arrays\n",
    "    states = np.array(records[\"states\"]).reshape((-1,1))\n",
    "    actions = np.array(records[\"actions\"]).reshape((-1,1))\n",
    "    rewards = np.array(records[\"rewards\"]).reshape((-1,1))\n",
    "\n",
    "    # compliment code states and actions\n",
    "    states_cc = compliment_code(states)\n",
    "    actions_cc = compliment_code(actions)\n",
    "    rewards_cc = compliment_code(rewards)\n",
    "\n",
    "    # states_fit, actions_fit, sarsa_rewards_fit = cls.calculate_SARSA(states_cc, actions_cc, rewards_cc, single_sample_reward=1.0)\n",
    "\n",
    "    # if FALCON has been previously trained\n",
    "    if hasattr(cls.fusion_art.modules[0], \"W\"):\n",
    "        # remove any duplicate clusters\n",
    "        cls = prune_clusters(cls)\n",
    "        # # shrink clusters to account for dynamic programming changes\n",
    "        # for m in range(3):\n",
    "        #     cls.fusion_art.modules[m] = cls.fusion_art.modules[m].shrink_clusters(shrink_ratio)\n",
    "\n",
    "    # fit FALCON to data\n",
    "    # data = cls.fusion_art.join_channel_data([states_fit, actions_fit, sarsa_rewards_fit])\n",
    "    # cls.fusion_art = cls.fusion_art.partial_fit(data)\n",
    "    cls = cls.partial_fit(states_cc, actions_cc, rewards_cc, single_sample_reward=1.0)\n",
    "    for m in range(3):\n",
    "        cls.fusion_art.modules[m] = cls.fusion_art.modules[m].shrink_clusters(shrink_ratio)\n",
    "\n",
    "    return cls\n",
    "\n",
    "def training_cycle(cls, epochs, steps, sarsa_alpha, sarsa_gamma, render_mode=None, shrink_ratio=0.1, explore_rate=0.0, checkpoint_frequency=50, early_stopping_reward=-20):\n",
    "    # create the environment\n",
    "    env = gym.make('CliffWalking-v0', render_mode=render_mode)\n",
    "    # define some constants\n",
    "    ACTION_SPACE = np.array([[0], [1.], [2.], [3.]])\n",
    "    STATE_MAX = 47\n",
    "    ACTION_MAX = 3\n",
    "    REWARD_MAX = 150\n",
    "    cls.td_alpha = sarsa_alpha\n",
    "    cls.td_lambda = sarsa_gamma\n",
    "\n",
    "    best_reward = -np.inf\n",
    "    best_cls = None\n",
    "\n",
    "    # track reward history\n",
    "    reward_history = []\n",
    "\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for e in pbar:\n",
    "        observation, info = env.reset()\n",
    "        n_observation = observation / STATE_MAX\n",
    "        records = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
    "        past_states = []\n",
    "        for _ in range(steps):\n",
    "            # get an action\n",
    "            observation_cc = compliment_code(np.array([n_observation]).reshape(1, -1))\n",
    "            if np.random.random() < explore_rate:\n",
    "                action = int(np.random.choice(ACTION_SPACE.flatten()))\n",
    "            else:\n",
    "                cls_action = cls.get_action(observation_cc, action_space=ACTION_SPACE, optimality=\"min\")\n",
    "                action = int(cls_action[0])\n",
    "            # normalize state and action\n",
    "            n_observation = observation / STATE_MAX\n",
    "            n_action = action / ACTION_MAX\n",
    "\n",
    "            # record state and action for training\n",
    "            records[\"states\"].append(n_observation)\n",
    "            records[\"actions\"].append(n_action)\n",
    "            past_states.append(observation)\n",
    "\n",
    "            # take a step\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # check reward value\n",
    "            if reward > -100:\n",
    "                # punish circular paths\n",
    "                if observation in past_states:\n",
    "                    reward = -2\n",
    "            # normalize and record reward from step\n",
    "            n_reward = abs(reward) / REWARD_MAX\n",
    "            records[\"rewards\"].append(n_reward)\n",
    "\n",
    "            # check if epoch is done\n",
    "            if terminated or truncated or reward == -100:\n",
    "                break\n",
    "\n",
    "        # train FALCON\n",
    "        cls = update_FALCON(records, cls, shrink_ratio)\n",
    "        # record sum of rewards generated during this epoch\n",
    "        reward_history.append(-sum(records[\"rewards\"])*REWARD_MAX)\n",
    "\n",
    "        # if this isnt random exploration\n",
    "        if explore_rate < 1.0:\n",
    "            # see if we should save a checkpoint\n",
    "            if (e+1)%checkpoint_frequency == 0 or e == epochs-1:\n",
    "                # test model\n",
    "                cls, test_reward_history = demo_cycle(cls, 1, steps, render_mode=None)\n",
    "                # check if our current model is better than the best previous model\n",
    "                if test_reward_history[0] >= best_reward or best_cls is None:\n",
    "                    # same a checkpoint\n",
    "                    best_cls = deepcopy(cls)\n",
    "                    best_reward = test_reward_history[0]\n",
    "                    # check early stopping condition\n",
    "                    if best_reward > early_stopping_reward:\n",
    "                        # show current best reward on progress bar\n",
    "                        pbar.set_postfix({'Best Reward': best_reward})\n",
    "                        return cls, reward_history\n",
    "                else:\n",
    "                    # restore previous best model\n",
    "                    cls = deepcopy(best_cls)\n",
    "            # show current best reward on progress bar\n",
    "            pbar.set_postfix({'Best Reward': best_reward})\n",
    "\n",
    "    env.close()\n",
    "    return cls, reward_history\n",
    "\n",
    "\n",
    "def demo_cycle(cls, epochs, steps, render_mode=None):\n",
    "    # create the environment\n",
    "    env = gym.make('CliffWalking-v0', render_mode=render_mode)\n",
    "    # define some constants\n",
    "    ACTION_SPACE = np.array([[0], [1.], [2.], [3.]])\n",
    "    STATE_MAX = 47\n",
    "    ACTION_MAX = 3\n",
    "    REWARD_MAX = 150\n",
    "\n",
    "    # track reward history\n",
    "    reward_history = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        observation, info = env.reset()\n",
    "        n_observation = observation / STATE_MAX\n",
    "        records = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
    "        past_states = []\n",
    "        for _ in range(steps):\n",
    "\n",
    "            # get an action\n",
    "            observation_cc = compliment_code(np.array([n_observation]).reshape(1, -1))\n",
    "            cls_action = cls.get_action(observation_cc, action_space=ACTION_SPACE, optimality=\"min\")\n",
    "            action = int(cls_action[0])\n",
    "\n",
    "            # normalize state and action\n",
    "            n_observation = observation / STATE_MAX\n",
    "            n_action = action / ACTION_MAX\n",
    "\n",
    "            # record state and action\n",
    "            records[\"states\"].append(n_observation)\n",
    "            records[\"actions\"].append(n_action)\n",
    "            past_states.append(observation)\n",
    "\n",
    "            # take a step\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # check reward value\n",
    "            if reward > -100:\n",
    "                # punish circular paths\n",
    "                if observation in past_states:\n",
    "                    reward = -2\n",
    "            # normalize and record reward from step\n",
    "            n_reward = abs(reward) / REWARD_MAX\n",
    "            records[\"rewards\"].append(n_reward)\n",
    "\n",
    "            # check if epoch is done\n",
    "            if terminated or truncated or reward == -100:\n",
    "                break\n",
    "        # record sum of rewards generated during this epoch\n",
    "        reward_history.append(-sum(records[\"rewards\"])*REWARD_MAX)\n",
    "\n",
    "    env.close()\n",
    "    return cls, reward_history\n",
    "\n",
    "\n",
    "def max_up_to_each_element(lst):\n",
    "    max_list = []\n",
    "    current_max = float('-inf')  # Start with the smallest possible value\n",
    "    for num in lst:\n",
    "        current_max = max(current_max, num)\n",
    "        max_list.append(current_max)\n",
    "    return max_list\n",
    "\n",
    "\n",
    "def train_FALCON():\n",
    "    # define training regimen\n",
    "    training_regimen = [\n",
    "        {\"name\": \"random\", \"epochs\": 1000, \"shrink_ratio\": 0.3, \"gamma\": 0.0, \"explore_rate\": 1.0, \"render_mode\": None},\n",
    "        {\"name\": \"explore 33%\", \"epochs\": 500, \"shrink_ratio\": 0.3, \"gamma\": 0.2, \"explore_rate\": 0.333, \"render_mode\": None},\n",
    "        {\"name\": \"explore 20%\", \"epochs\": 500, \"shrink_ratio\": 0.3, \"gamma\": 0.2, \"explore_rate\": 0.20, \"render_mode\": None},\n",
    "        {\"name\": \"explore 5%\", \"epochs\": 1000, \"shrink_ratio\": 0.3, \"gamma\": 0.2, \"explore_rate\": 0.05, \"render_mode\": None},\n",
    "    ]\n",
    "    MAX_STEPS = 25\n",
    "    SARSA_ALPHA = 1.0\n",
    "\n",
    "    # define parameters for state, action, and reward modules\n",
    "    art_state = FuzzyART(rho=0.99,alpha=0.01, beta=1.0)\n",
    "    art_action = FuzzyART(rho=0.99,alpha=0.01, beta=1.0)\n",
    "    art_reward = FuzzyART(rho=0.0,alpha=0.01, beta=1.0)\n",
    "    # instantiate FALCON\n",
    "    cls = TD_FALCON(art_state, art_action, art_reward, channel_dims=[2, 2, 2])\n",
    "    # record rewards for each epoch\n",
    "    all_rewards = []\n",
    "    # initialize FALCON with random exploration\n",
    "    print(\"Starting Training\")\n",
    "    for regimen in training_regimen:\n",
    "        print(f\"Starting Training Cycle: {regimen['name']}\")\n",
    "        cls, reward_history = training_cycle(\n",
    "            cls,\n",
    "            epochs=regimen[\"epochs\"],\n",
    "            steps=MAX_STEPS,\n",
    "            sarsa_alpha=SARSA_ALPHA,\n",
    "            sarsa_gamma=regimen[\"gamma\"],\n",
    "            render_mode=regimen[\"render_mode\"],\n",
    "            shrink_ratio=regimen[\"shrink_ratio\"],\n",
    "            explore_rate=regimen[\"explore_rate\"]\n",
    "        )\n",
    "        all_rewards.extend(reward_history)\n",
    "        print(\"MAX REWARD:\",max(reward_history))\n",
    "\n",
    "    # demo learned policy\n",
    "    cls, reward_history = demo_cycle(cls, epochs=2, steps=25, render_mode=\"human\")\n",
    "    print(reward_history)\n",
    "    all_rewards.extend(reward_history)\n",
    "    max_rewards = max_up_to_each_element(all_rewards)\n",
    "\n",
    "    # plot reward history\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(all_rewards))), all_rewards, \"r-\")\n",
    "    plt.plot(list(range(len(all_rewards))), max_rewards, \"b-\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Rewards over Time\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This takes approximately 3 minutes\n",
    "    np.random.seed(42)\n",
    "    train_FALCON()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
